{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saadkhi/GSoC-2025-Task/blob/main/Specific_Test_I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 1 (Use 1 PDF for training model)**"
      ],
      "metadata": {
        "id": "CDQmfNZBOoix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth, drive\n",
        "import gdown\n",
        "import os\n",
        "\n",
        "# Authentication for Google Drive\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Folder ID from the shared link\n",
        "folder_id = \"1-91y1fQHanXfzx5WUy0qLroflfgVnk_L\"\n",
        "\n",
        "# Destination in your Google Drive\n",
        "destination = \"/content/drive/My Drive/dataset\"\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "os.makedirs(destination, exist_ok=True)\n",
        "\n",
        "# Use gdown to download the folder\n",
        "gdown.download_folder(f\"https://drive.google.com/drive/folders/{folder_id}\", output=destination, quiet=False, use_cookies=False)\n",
        "\n",
        "print(\"Download complete! Files saved in:\", destination)"
      ],
      "metadata": {
        "id": "D-feVZqnTzYg",
        "outputId": "65e25a6f-65b0-46cb-e0c2-073b5b547520",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder 1-ATumAsFodV9Yjb4sChmslZ45HKtW9pX images\n",
            "Processing file 1-JaE5tNwyNHGDG60GHVihshifrVGgRKj Ezcaray - Vozes.pdf\n",
            "Processing file 1-7pV-zFu6cVuty3wRhjMTN3XbpBhQzHC Mendo - Principe perfecto.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-JaE5tNwyNHGDG60GHVihshifrVGgRKj\n",
            "To: /content/drive/My Drive/dataset/Ezcaray - Vozes.pdf\n",
            "100%|██████████| 3.44M/3.44M [00:00<00:00, 46.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-7pV-zFu6cVuty3wRhjMTN3XbpBhQzHC\n",
            "To: /content/drive/My Drive/dataset/Mendo - Principe perfecto.pdf\n",
            "100%|██████████| 2.37M/2.37M [00:00<00:00, 60.3MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete! Files saved in: /content/drive/My Drive/dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdf2image\n",
        "!apt-get install -y poppler-utils\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from pdf2image import convert_from_path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import jaccard_score, f1_score, accuracy_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e951zZPnM_JM",
        "outputId": "6feee335-6ab8-44b3-ec5b-970fe6f77224"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.11/dist-packages (1.17.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.1.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "pdf_folder_path = \"/content/drive/My Drive/dataset/\"  # Folder containing PDFs\n",
        "output_image_folder = \"/content/pdf_images\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "if not os.path.exists(output_image_folder):\n",
        "    os.makedirs(output_image_folder)\n",
        "\n",
        "# Function to convert PDF to images\n",
        "def pdf_to_images(pdf_path, output_folder):\n",
        "    images = convert_from_path(pdf_path)\n",
        "    base_filename = os.path.splitext(os.path.basename(pdf_path))[0] # get filename without extension\n",
        "    for i, image in enumerate(images):\n",
        "        image.save(os.path.join(output_folder, f\"{base_filename}_page_{i}.jpg\"), \"JPEG\")\n",
        "\n",
        "# Convert all PDFs to images\n",
        "pdf_files = [f for f in os.listdir(pdf_folder_path) if f.endswith(\".pdf\")]\n",
        "for pdf_file in pdf_files:\n",
        "    pdf_path = os.path.join(pdf_folder_path, pdf_file)\n",
        "    pdf_to_images(pdf_path, output_image_folder)\n",
        "\n",
        "# Load images and masks\n",
        "image_paths = [os.path.join(output_image_folder, f) for f in os.listdir(output_image_folder) if f.endswith(\".jpg\")]\n",
        "images = [cv2.imread(path) for path in image_paths]\n",
        "\n",
        "# Function to create simple masks (replace with your actual mask creation)\n",
        "def create_masks(image_paths):\n",
        "    masks = []\n",
        "    for image_path in image_paths:\n",
        "        img = cv2.imread(image_path)\n",
        "        mask = np.zeros_like(img[:, :, 0], dtype=np.uint8)\n",
        "        mask[100:img.shape[0]-100, 100:img.shape[1]-100] = 255\n",
        "        masks.append(mask)\n",
        "    return masks\n",
        "\n",
        "masks = create_masks(image_paths)\n",
        "\n",
        "# Normalize images and create numpy arrays\n",
        "images = [img / 255.0 for img in images]\n",
        "resized_images = [cv2.resize(img, (256, 256)) for img in images]\n",
        "resized_masks = [cv2.resize(mask, (256, 256), interpolation=cv2.INTER_NEAREST) / 255.0 for mask in masks]\n",
        "X = np.array(resized_images)\n",
        "y = np.array(resized_masks)[:, :, :, np.newaxis]\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ImageDataGenerator for augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# U-Net model\n",
        "def build_unet(input_shape=(256, 256, 3)):\n",
        "    inputs = layers.Input(input_shape)\n",
        "    # Encoder\n",
        "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
        "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
        "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    # Bottleneck\n",
        "    conv_mid = layers.Conv2D(256, 3, activation='relu', padding='same')(pool1)\n",
        "    conv_mid = layers.Conv2D(256, 3, activation='relu', padding='same')(conv_mid)\n",
        "    # Decoder\n",
        "    up7 = layers.Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(conv_mid)\n",
        "    merge7 = layers.concatenate([up7, conv1], axis=3)\n",
        "    conv7 = layers.Conv2D(64, 3, activation='relu', padding='same')(merge7) # Corrected line\n",
        "    conv7 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv7)\n",
        "    outputs = layers.Conv2D(1, 1, activation='sigmoid')(conv7)\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = build_unet()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with ImageDataGenerator\n",
        "model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
        "          steps_per_epoch=int(len(X_train) / 32),\n",
        "          epochs=10,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(np.uint8)\n",
        "\n",
        "# Calculate IoU and F1-score\n",
        "iou = jaccard_score(y_test.flatten(), y_pred_binary.flatten())\n",
        "f1 = f1_score(y_test.flatten(), y_pred_binary.flatten())\n",
        "\n",
        "print(f\"Test IoU: {iou}, Test F1-Score: {f1}\")"
      ],
      "metadata": {
        "id": "OjDDCWtfDfNp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc17f033-22c7-48e8-a466-069550b1ab14"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.6058 - loss: 0.6919 - val_accuracy: 0.8034 - val_loss: 0.6001\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709ms/step - accuracy: 0.8038 - loss: 0.5936 - val_accuracy: 0.8034 - val_loss: 0.5280\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 916ms/step - accuracy: 0.8038 - loss: 0.5177 - val_accuracy: 0.8034 - val_loss: 0.6263\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.8038 - loss: 0.6089 - val_accuracy: 0.8034 - val_loss: 0.5206\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 908ms/step - accuracy: 0.8038 - loss: 0.5115 - val_accuracy: 0.8034 - val_loss: 0.5243\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.8038 - loss: 0.5211 - val_accuracy: 0.8034 - val_loss: 0.5317\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 678ms/step - accuracy: 0.8038 - loss: 0.5292 - val_accuracy: 0.8034 - val_loss: 0.5210\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.8038 - loss: 0.5214 - val_accuracy: 0.8034 - val_loss: 0.5057\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step - accuracy: 0.8038 - loss: 0.5036 - val_accuracy: 0.8034 - val_loss: 0.5074\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 657ms/step - accuracy: 0.8038 - loss: 0.5033 - val_accuracy: 0.8034 - val_loss: 0.5125\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351ms/step\n",
            "Test IoU: 0.803375244140625, Test F1-Score: 0.8909684734232481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test.flatten(), y_pred_binary.flatten())\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "oCPx2VHNYDuy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "980c7bfe-8187-4b87-c1de-c2bc162e2a59"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.803375244140625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 1 (Use all PDF's for training model)**"
      ],
      "metadata": {
        "id": "3YNGE2db8399"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth, drive\n",
        "import gdown\n",
        "import os\n",
        "\n",
        "# Authentication for Google Drive\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Folder ID from the shared link\n",
        "folder_id = \"1acoMZD4i2OTYFcoRHfH3_INITYPvN-UR\"\n",
        "\n",
        "# Destination in your Google Drive\n",
        "destination = \"/content/drive/My Drive/dataset_all\"\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "os.makedirs(destination, exist_ok=True)\n",
        "\n",
        "# Use gdown to download the folder\n",
        "gdown.download_folder(f\"https://drive.google.com/drive/folders/{folder_id}\", output=destination, quiet=False, use_cookies=False)\n",
        "\n",
        "print(\"Download complete! Files saved in:\", destination)"
      ],
      "metadata": {
        "id": "Rbb6FC989g59",
        "outputId": "58c69edd-c657-497a-dba9-305223b26cdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 1IuFzvkG8eKQQYV2BHQp-PRAgZNSbTTkB Buendia - Instruccion.pdf\n",
            "Processing file 1hFA6Xl6qSTMWHdWofO8M5vXr4dlwFrx4 Constituciones sinodales Calahorra 1602.pdf\n",
            "Processing file 1NIq9VdyuQQwmlptcdwcfVyHKtFfiRKxr ES-AHPHU - J-000312-0014 – 1579.pdf\n",
            "Processing file 15acKc0qnG_OiGxLigIJVAknlOSQ5S4Hu Ezcaray - Vozes.pdf\n",
            "Processing file 17MCzlffI2JavfKE4M4yqGVnP814p2SNH J&#x3a;0017&#x3a;03-J&#x3a;0085&#x3a;11 – 1799-1845.pdf\n",
            "Processing file 1P4bXC8olGpZQ2Dp-azT-W3XG0HWuxV2I Mendo - Principe perfecto.pdf\n",
            "Processing file 1tkeFJLLHOzaKxef7FAzyTmVVpfd394aJ Paredes - Reglas generales.pdf\n",
            "Processing file 1yDoblfJxzM906V07RxzcNy5w303jyQHo PORCONES.228.35 – 1636.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1IuFzvkG8eKQQYV2BHQp-PRAgZNSbTTkB\n",
            "To: /content/drive/My Drive/dataset_all/Buendia - Instruccion.pdf\n",
            "100%|██████████| 3.29M/3.29M [00:00<00:00, 161MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hFA6Xl6qSTMWHdWofO8M5vXr4dlwFrx4\n",
            "To: /content/drive/My Drive/dataset_all/Constituciones sinodales Calahorra 1602.pdf\n",
            "100%|██████████| 1.81M/1.81M [00:00<00:00, 122MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NIq9VdyuQQwmlptcdwcfVyHKtFfiRKxr\n",
            "To: /content/drive/My Drive/dataset_all/ES-AHPHU - J-000312-0014 – 1579.pdf\n",
            "100%|██████████| 8.44M/8.44M [00:00<00:00, 134MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15acKc0qnG_OiGxLigIJVAknlOSQ5S4Hu\n",
            "To: /content/drive/My Drive/dataset_all/Ezcaray - Vozes.pdf\n",
            "100%|██████████| 3.44M/3.44M [00:00<00:00, 171MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17MCzlffI2JavfKE4M4yqGVnP814p2SNH\n",
            "To: /content/drive/My Drive/dataset_all/J&#x3a;0017&#x3a;03-J&#x3a;0085&#x3a;11 – 1799-1845.pdf\n",
            "100%|██████████| 12.6M/12.6M [00:00<00:00, 176MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1P4bXC8olGpZQ2Dp-azT-W3XG0HWuxV2I\n",
            "To: /content/drive/My Drive/dataset_all/Mendo - Principe perfecto.pdf\n",
            "100%|██████████| 2.37M/2.37M [00:00<00:00, 124MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tkeFJLLHOzaKxef7FAzyTmVVpfd394aJ\n",
            "To: /content/drive/My Drive/dataset_all/Paredes - Reglas generales.pdf\n",
            "100%|██████████| 3.82M/3.82M [00:00<00:00, 148MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1yDoblfJxzM906V07RxzcNy5w303jyQHo\n",
            "To: /content/drive/My Drive/dataset_all/PORCONES.228.35 – 1636.pdf\n",
            "100%|██████████| 37.9M/37.9M [00:00<00:00, 129MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete! Files saved in: /content/drive/My Drive/dataset_all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdf2image\n",
        "!apt-get install -y poppler-utils\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from pdf2image import convert_from_path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import jaccard_score, f1_score, accuracy_score"
      ],
      "metadata": {
        "id": "EhgIxW_P-Ln2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17332295-48a9-464d-b561-8aa57e6791c2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.11/dist-packages (1.17.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.1.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "pdf_folder_path = \"/content/drive/My Drive/dataset_all/\"  # Folder containing PDFs\n",
        "output_image_folder = \"/content/pdf_images\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "if not os.path.exists(output_image_folder):\n",
        "    os.makedirs(output_image_folder)\n",
        "\n",
        "# Function to convert PDF to images\n",
        "def pdf_to_images(pdf_path, output_folder):\n",
        "    images = convert_from_path(pdf_path)\n",
        "    base_filename = os.path.splitext(os.path.basename(pdf_path))[0] # get filename without extension\n",
        "    for i, image in enumerate(images):\n",
        "        image.save(os.path.join(output_folder, f\"{base_filename}_page_{i}.jpg\"), \"JPEG\")\n",
        "\n",
        "# Convert all PDFs to images\n",
        "pdf_files = [f for f in os.listdir(pdf_folder_path) if f.endswith(\".pdf\")]\n",
        "for pdf_file in pdf_files:\n",
        "    pdf_path = os.path.join(pdf_folder_path, pdf_file)\n",
        "    pdf_to_images(pdf_path, output_image_folder)\n",
        "\n",
        "# Load images and masks\n",
        "image_paths = [os.path.join(output_image_folder, f) for f in os.listdir(output_image_folder) if f.endswith(\".jpg\")]\n",
        "images = [cv2.imread(path) for path in image_paths]\n",
        "\n",
        "# Function to create simple masks (replace with your actual mask creation)\n",
        "def create_masks(image_paths):\n",
        "    masks = []\n",
        "    for image_path in image_paths:\n",
        "        img = cv2.imread(image_path)\n",
        "        mask = np.zeros_like(img[:, :, 0], dtype=np.uint8)\n",
        "        mask[100:img.shape[0]-100, 100:img.shape[1]-100] = 255\n",
        "        masks.append(mask)\n",
        "    return masks\n",
        "\n",
        "masks = create_masks(image_paths)\n",
        "\n",
        "# Normalize images and create numpy arrays\n",
        "images = [img / 255.0 for img in images]\n",
        "resized_images = [cv2.resize(img, (256, 256)) for img in images]\n",
        "resized_masks = [cv2.resize(mask, (256, 256), interpolation=cv2.INTER_NEAREST) / 255.0 for mask in masks]\n",
        "X = np.array(resized_images)\n",
        "y = np.array(resized_masks)[:, :, :, np.newaxis]\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ImageDataGenerator for augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# U-Net model\n",
        "def build_unet(input_shape=(256, 256, 3)):\n",
        "    inputs = layers.Input(input_shape)\n",
        "    # Encoder\n",
        "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
        "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
        "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    # Bottleneck\n",
        "    conv_mid = layers.Conv2D(256, 3, activation='relu', padding='same')(pool1)\n",
        "    conv_mid = layers.Conv2D(256, 3, activation='relu', padding='same')(conv_mid)\n",
        "    # Decoder\n",
        "    up7 = layers.Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(conv_mid)\n",
        "    merge7 = layers.concatenate([up7, conv1], axis=3)\n",
        "    conv7 = layers.Conv2D(64, 3, activation='relu', padding='same')(merge7) # Corrected line\n",
        "    conv7 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv7)\n",
        "    outputs = layers.Conv2D(1, 1, activation='sigmoid')(conv7)\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Compile the model\n",
        "model = build_unet()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with ImageDataGenerator\n",
        "model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
        "          steps_per_epoch=int(len(X_train) / 32),\n",
        "          epochs=10,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(np.uint8)\n",
        "\n",
        "# Calculate IoU and F1-score\n",
        "iou = jaccard_score(y_test.flatten(), y_pred_binary.flatten())\n",
        "f1 = f1_score(y_test.flatten(), y_pred_binary.flatten())\n",
        "\n",
        "print(f\"Test IoU: {iou}, Test F1-Score: {f1}\")"
      ],
      "metadata": {
        "id": "-GDFqs478hXZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}